# Exploring LLM API Parameters

This repository contains a Python script and analysis for experimenting with key parameters of Large Language Models (LLMs) using the DeepSeek API. The project investigates how parameters like temperature, top-k sampling, and max tokens influence model responses.

## Table of Contents
- [Project Overview](#project-overview)
- [Key Features](#key-features)
- [Installation](#installation)
- [Usage](#usage)
- [Parameters Tested](#parameters-tested)
- [Results](#results)
- [Contributing](#contributing)
- [License](#license)
- [Acknowledgements](#acknowledgements)

## Project Overview
This assignment explores how LLM API parameters affect text generation quality and behavior. The script interacts with the DeepSeek API to:
- Analyze the impact of **temperature** on response randomness
- Evaluate **top-k sampling** for vocabulary control
- Test **max_tokens** for response length management
- Generate visualizations of parameter effects

## Key Features
- 🎛️ API integration with parameter tuning
- 📊 Matplotlib visualizations (response length vs. parameters)
- 📝 Observation report with qualitative analysis
- 🔍 Examples of model outputs across configurations

## Installation
1. Clone the repository:
   ```bash
   git clone https://github.com/yourusername/llm-parameters-deepseek.git
   cd llm-parameters-deepseek
